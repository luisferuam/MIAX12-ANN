{"cells":[{"cell_type":"markdown","metadata":{"id":"KoFZV1kBZkAi"},"source":["<font color=\"#CA3532\"><h1 align=\"left\">Inteligencia Artificial Aplicada a la Bolsa (MIAX-12)</h1></font>\n","<font color=\"#5b5a59\"><h2 align=\"left\">Extensión del modelo de regresión lineal a una dimensión arbitraria</h2></font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"74sT4NIPYtiy"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["# Datos sintéticos"],"metadata":{"id":"ki2-Ji49VcIG"}},{"cell_type":"markdown","metadata":{"id":"GkQ5ae7LZ-zc"},"source":["Generación de los datos del problema:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3__RYeMyZyEg"},"outputs":[],"source":["# Parametros:\n","d = 5 # Dimension del problema\n","w = np.random.randn(1, d)\n","b = 1.0\n","xmin = 0.0\n","xmax = 10.0\n","noise = 1.0\n","n = 1000\n","\n","# Datos del problema generados al azar:\n","x = xmin + np.random.rand(n, d)*(xmax - xmin)\n","t0 = np.dot(x, w.T) + b\n","t = t0 + np.random.randn(n, 1)*noise\n","tmin = np.min(t)\n","tmax = np.max(t)\n","\n","# Distribucion de las dos primeras variables:\n","plt.figure(figsize=(12,6))\n","plt.subplot(121)\n","plt.plot(x[:, 0], x[:, 1], 'o')\n","plt.grid(True)\n","plt.xlabel(\"x1\")\n","plt.ylabel(\"x2\")\n","plt.subplot(122)\n","\n","# Grafica de t frente a t0:\n","plt.plot(t0, t, 'o')\n","plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n","plt.grid(True)\n","plt.xlabel(\"t0\")\n","plt.ylabel(\"t\")\n","plt.show()\n","\n","# Error esperado:\n","e = np.sum((t-t0)*(t-t0))\n","print(\"Error esperado = %f\" % e)"]},{"cell_type":"markdown","metadata":{"id":"nax9A91aaNc0"},"source":["Forma de los vectores:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGYdnMLDZ4nG"},"outputs":[],"source":["print(x.shape)\n","print(t.shape)"]},{"cell_type":"markdown","metadata":{"id":"WRgATzqpad5m"},"source":["Modelo de regresión lineal con los parámetros inicializados al azar:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrbCLsvPaTRI"},"outputs":[],"source":["w = np.random.randn(1, d)\n","b = np.random.randn()\n","\n","# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n","y = np.dot(x, w.T) + b\n","\n","# Grafica de y frente a t:\n","plt.figure(figsize=(6, 6))\n","plt.plot(t, y, 'o')\n","plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n","plt.grid(True)\n","plt.xlabel(\"t\")\n","plt.ylabel(\"y\")\n","plt.show()\n","\n","# Error:\n","e = np.sum((y-t)*(y-t))\n","print(\"Error = %f\" % e)"]},{"cell_type":"markdown","metadata":{"id":"Ad_HnpDMas0N"},"source":["Entrenamiento del modelo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iAWYGrCDah2I"},"outputs":[],"source":["nepocas = 64\n","eta = 0.000005\n","\n","plt.figure(figsize=(16,16))\n","\n","k = 1\n","error = []\n","for epoch in range(nepocas):\n","    y = np.dot(x, w.T) + b\n","\n","    #----------------------------------------------------------\n","    # TO-DO: Calcula el error:\n","    e = 0 # TO-DO actualiza el error\n","    #----------------------------------------------------------\n","    error.append(e)\n","\n","    if epoch%4 == 0:\n","        plt.subplot(4, 4, k)\n","        plt.plot(t, y, 'o')\n","        plt.plot([tmin, tmax], [tmin, tmax], 'r-')\n","        plt.grid(True)\n","        plt.title(\"epoca = %d, err = %.2f\" % (epoch, e))\n","        k += 1\n","\n","    #----------------------------------------------------------\n","    # TO-DO: Calcula los gradientes y actualiza los parametros:\n","    b = b # TO-DO actualiza los parámetros\n","    w = w # TO-DO actualiza los parámetros\n","    #----------------------------------------------------------\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"93E3pGM5a51W"},"source":["Error frente a número de épocas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzR5BJdWaubX"},"outputs":[],"source":["plt.plot(range(nepocas), error)\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"7uwP6KLQc2qn"},"source":["# California Housing"]},{"cell_type":"markdown","metadata":{"id":"2PXIq1nYRckw"},"source":["California Housing contiene información sobre viviendas en distintas áreas de California, basada en el censo de 1990. El objetivo consiste en predecir el valor medio de las viviendas en cada área. Los atributos son los siguientes:\n","\n","* **MedInc**: Ingreso medio de los hogares dentro de un área\n","\n","* **HouseAge**: Edad media de una vivienda dentro de un área\n","\n","* **AveRooms**: Promedio de habitaciones en una vivienda dentro de un área\n","\n","* **AveBedrms**: Promedio de dormitorios en una vivienda dentro de un área\n","\n","* **Population**: Población en el área\n","\n","* **AveOccup**: Tamaño de familia promedio en el área\n","\n","* **Latitude**: Latitud\n","\n","* **Longitude**: Longitud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HgfkFCxOc4jR"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","\n","x, t = fetch_california_housing(return_X_y=True, as_frame=True)\n","print(x.shape)\n","print(t.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XvOJX9p8Rckx"},"outputs":[],"source":["x.head()"]},{"cell_type":"markdown","metadata":{"id":"dSFaYz_nRckx"},"source":["Separamos en train - test para evaluar:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4i0MHdGRckx"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, t, test_size=0.2, random_state=42)\n","\n","print(\"TRAIN\", x_train.shape, y_train.shape)\n","print(\"TEST\", x_test.shape, y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"Guhb2FZlSqSt"},"source":["Estandarizamos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKz0eODfSvOJ"},"outputs":[],"source":["# Estandarizar los datos:\n","medias = x_train.mean()\n","stds = x_train.std()\n","x_train = (x_train - medias) / stds\n","x_test = (x_test - medias) / stds"]},{"cell_type":"markdown","metadata":{"id":"1gLrOZoRRdSP"},"source":["Nos quedamos sólo con los primeros 1000 ejemplos para que las ejecuciones sean más rápidas."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPx-zvLLRfuZ"},"outputs":[],"source":["x_train = x_train[:1000]\n","y_train = y_train[:1000]"]},{"cell_type":"markdown","source":["Para no tener problemas con el tipo de dato, vamos a trabajar siempre con numpy."],"metadata":{"id":"iI9AriBkTw70"}},{"cell_type":"code","source":["x_train = x_train.values\n","y_train = y_train.values\n","x_test = x_test.values\n","y_test = y_test.values"],"metadata":{"id":"oa0v6VIgTwO4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZWBjllTLeCNn"},"source":["Construye un modelo de regresión lineal para predecir la variable $t$ a partir de los atributos $x$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKvai5ojitYR"},"outputs":[],"source":["d = x_train.shape[1]\n","\n","w = np.random.randn(1, d)\n","b = np.random.randn()\n","\n","nepocas = 400\n","lr = 0.0001\n","\n","k = 1\n","error = []\n","for i in range(nepocas):\n","    pred = np.dot(x_train, w.T) + b\n","    pred = pred[:, 0]\n","\n","    #----------------------------------------------------------\n","    # TO-DO: Calcula el error:\n","    e = 0 # TO-DO actualiza el error\n","    #----------------------------------------------------------\n","    error.append(e)\n","\n","    #----------------------------------------------------------\n","    # TO-DO: Calcula los gradientes y actualiza los parametros:\n","    b = b # TO-DO actualiza los parámetros\n","    w = w # TO-DO actualiza los parámetros\n","    #----------------------------------------------------------\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_qbIXmClZix"},"outputs":[],"source":["plt.plot(range(nepocas), error)\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFzKmKyKrnud","scrolled":false},"outputs":[],"source":["# Aplico el modelo a los datos y comparo la prediccion y con el objetivo t:\n","pred = np.dot(x_test, w.T) + b\n","pred = pred[:, 0]\n","\n","# Grafica de y frente a t:\n","plt.figure(figsize=(6, 6))\n","plt.plot(y_test, pred, 'o')\n","plt.plot([0, 6], [0, 6], 'r-')\n","plt.grid(True)\n","plt.xlabel(\"y_test\")\n","plt.ylabel(\"pred\")\n","plt.show()\n","\n","# Error:\n","e = np.mean((pred-y_test)*(pred-y_test))\n","print(\"Error = %f\" % e)"]},{"cell_type":"markdown","metadata":{"id":"6vfepGOjVigB"},"source":["## Comprobación de California Housing con otras alternativas\n","\n","Vamos a comprobarlo con la regresión lineal implementada con Sklearn:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S94yIfTMVYyF"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error"]},{"cell_type":"markdown","metadata":{"id":"SrwPMPXCWKO2"},"source":["### Opción 1: Regresión lineal siguiendo la estrategia de mínimos cuadrados\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DrdPe1gIVe5X"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAAX4yEQVdjm"},"outputs":[],"source":["lr = LinearRegression().fit(x_train, y_train)\n","pred = lr.predict(x_train)\n","print(\"TRAIN:\", mean_squared_error(y_train, pred))\n","pred = lr.predict(x_test)\n","print(\"TEST:\", mean_squared_error(y_test, pred))"]},{"cell_type":"markdown","metadata":{"id":"6iYZ33k2WcFj"},"source":["### Opción 2: Regresión lineal siguiendo la estrategia de minimización mediante descenso por gradiente (SGD)\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBUJQBNlVzFF"},"outputs":[],"source":["from sklearn.linear_model import SGDRegressor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QAg3en1jXAZ9"},"outputs":[],"source":["lr = SGDRegressor(penalty=None).fit(x_train, y_train)\n","pred = lr.predict(x_train)\n","print(\"TRAIN:\", mean_squared_error(y_train, pred))\n","pred = lr.predict(x_test)\n","print(\"TEST:\", mean_squared_error(y_test, pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnoSmTFBHRQ_"},"outputs":[],"source":["# La variable coef_ representa los pesos de cada atributo\n","lr.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A1AF3AgwHalM"},"outputs":[],"source":["# La variable intercept_ representa el bias\n","lr.intercept_"]},{"cell_type":"markdown","metadata":{"id":"mWlisovrGpt2"},"source":["## Probemos la regresión lineal con regularización L1 o L2"]},{"cell_type":"markdown","metadata":{"id":"PUtgGEUEGzYS"},"source":["### Regularización Ridge (L2)"]},{"cell_type":"markdown","metadata":{"id":"1zUHpod-Jvqu"},"source":["La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8S0P_pwHmP7"},"outputs":[],"source":["lr = SGDRegressor(penalty='l2', alpha=0.1).fit(x_train, y_train)\n","pred = lr.predict(x_train)\n","print(\"TRAIN:\", mean_squared_error(y_train, pred))\n","pred = lr.predict(x_test)\n","print(\"TEST:\", mean_squared_error(y_test, pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2P6c3a9H6i2"},"outputs":[],"source":["# La variable coef_ guarda todos los pesos de la regresión\n","lr.coef_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p74qNUoYIBNf"},"outputs":[],"source":["# La variable intercept_ guarda el bias\n","lr.intercept_"]},{"cell_type":"markdown","metadata":{"id":"SK0SvNYtJ6TG"},"source":["Pintemos los pesos de los atributos con un diagrama de barras. Así podremos saber qué \"importancia\" le da nuestro modelo a los atributos de entrada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UBpxBjrNI70z"},"outputs":[],"source":["plt.bar(np.arange(len(lr.coef_)), lr.coef_)\n","plt.xlabel(\"Attribute ID\")\n","plt.ylabel(\"Weight\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OBiuzHxYKUp5"},"source":["Veamos ahora el error frente al factor alpha de regularización L2:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfj4o4PAIB3G"},"outputs":[],"source":["l2_values = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n","errores = []\n","for l2 in l2_values:\n","    lr = SGDRegressor(penalty='l2', alpha=l2).fit(x_train, y_train)\n","    pred = lr.predict(x_test)\n","    e = mean_squared_error(y_test, pred)\n","    errores.append(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Jd8dWBEIpG-"},"outputs":[],"source":["plt.plot(errores, '.-')\n","plt.xticks(np.arange(len(l2_values)), l2_values)\n","plt.xlabel(\"Factor de regularización L2\")\n","plt.ylabel(\"MSE\")\n","plt.show()\n","\n","# En la figura vemos que cuanto mayor penalización damos a los pesos, mayor es el MSE"]},{"cell_type":"markdown","metadata":{"id":"fyP5NIFRKmnZ"},"source":["### Regularización Lasso (L1)"]},{"cell_type":"markdown","metadata":{"id":"k_ij1JflKtAz"},"source":["La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njpzN168Iqxy"},"outputs":[],"source":["lr = SGDRegressor(penalty='l1', alpha=0.1).fit(x_train, y_train)\n","pred = lr.predict(x_test)\n","mean_squared_error(y_test, pred)"]},{"cell_type":"markdown","metadata":{"id":"1WIsnPn8Kz-M"},"source":["Pintemos los pesos de los atributos con un diagrama de barras. Así podremos saber qué \"importancia\" le da nuestro modelo a los atributos de entrada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N84Ko1oeKvTU"},"outputs":[],"source":["plt.bar(np.arange(len(lr.coef_)), lr.coef_)\n","plt.xlabel(\"Attribute ID\")\n","plt.ylabel(\"Weight\")\n","plt.show()\n","\n","# Con la regularización L1 (mucho más agresiva que la L2) vemos como los pesos de algunos atributos tienden a 0,\n","# es decir, el modelo aprende a realizar una selección de características de entrada."]},{"cell_type":"markdown","metadata":{"id":"etVhLe_0K9kV"},"source":["Veamos ahora el error frente al factor alpha de regularización L1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_gd_pDiK1nT"},"outputs":[],"source":["l1_values = [0.0, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1.0]\n","errores = []\n","for l1 in l1_values:\n","    lr = SGDRegressor(penalty='l1', alpha=l1).fit(x_train, y_train)\n","    pred = lr.predict(x_test)\n","    e = mean_squared_error(y_test, pred)\n","    errores.append(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aTe4u7uaLMc5"},"outputs":[],"source":["plt.plot(errores, '.-')\n","plt.xticks(np.arange(len(l1_values)), l1_values)\n","plt.xlabel(\"Factor de regularización L1\")\n","plt.ylabel(\"MSE\")\n","plt.show()\n","\n","# En la figura vemos que cuanto mayor penalización damos a los pesos, mayor es el MSE\n","# Ojo en la escala. Fíjate además como L1, comparada con L2, es más agresiva."]},{"cell_type":"markdown","metadata":{"id":"iclJhaTFRck9"},"source":["### ElasticNet"]},{"cell_type":"markdown","metadata":{"id":"Typ17vsjRck-"},"source":["La clase ```SGDRegressor``` implementa la opción de penalizar tanto con L1 como con L2 el entrenamiento de descenso por gradiente. Si quieres combinar ambas regularizaciones, tienes la opción de utilizar la configuración *ElasticNet*, cuya penalización se expresa de la siguiente manera:\n","\n","$$Penalización ElasticNet = \\alpha (\\phi L1 + (1 - \\phi) L2)$$\n","\n","donde $\\alpha$ es el factor de regularización (parámetro `alpha`) y $\\phi$ es el parámetro que define la contribución de las penalizaciones (parámetro `l1_ratio`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-MrafHqRck-"},"outputs":[],"source":["lr = SGDRegressor(penalty='elasticnet', alpha=0.1, l1_ratio=0.15).fit(x_train, y_train)\n","pred = lr.predict(x_test)\n","mean_squared_error(y_test, pred)"]},{"cell_type":"markdown","metadata":{"id":"ScQx4nlARck-"},"source":["**Ejercicio**: Busca la mejor configuración posible combinando los hiperparámetros alpha y l1_ratio"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BkKw_yYRck-"},"outputs":[],"source":["#------------------------------------------------------------------\n","# TO-DO: Define los posibles valores de alpha y l1_ratio a buscar\n","ratio_values = []\n","alpha_values = []\n","#------------------------------------------------------------------\n","\n","errores = []\n","#------------------------------------------------------------------\n","# TO-DO: Construye un bucle para probar todas las posibilidades\n","\n","#------------------------------------------------------------------\n","\n","errores = np.array(errores)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mi3nKqHURck_"},"outputs":[],"source":["plt.imshow(errores)\n","plt.xticks(range(len(ratio_values)), np.around(ratio_values, 2), rotation=90)\n","plt.xlabel(\"Ratio L1 (phi)\")\n","plt.yticks(range(len(alpha_values)), alpha_values)\n","plt.ylabel(\"Regularization penalty (alpha)\")\n","plt.colorbar()\n","plt.show()\n","print(\"MIN ERROR:\", errores.min())\n","print(\"ALPHA:\", alpha_values[np.where(errores == errores.min())[0][0]])\n","print(\"RATIO:\", ratio_values[np.where(errores == errores.min())[1][0]])"]},{"cell_type":"markdown","metadata":{"id":"pUDbIuQ5LVm2"},"source":["## ¿Y si utilizamos métodos de Kernel para buscar transformaciónes no lineales?\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzhBA79lOyBX"},"outputs":[],"source":["from sklearn.kernel_ridge import KernelRidge"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hAkrajXpPDcR"},"outputs":[],"source":["lr = KernelRidge(alpha=0.1, kernel='rbf', gamma=None).fit(x_train, y_train)\n","pred = lr.predict(x_test)\n","mean_squared_error(y_test, pred)\n","\n","# Al encontrar transformaciones no lineales, el modelo obtiene un error mucho menor"]},{"cell_type":"markdown","source":["# Alpha y Beta mediante regresión lineal"],"metadata":{"id":"uFIEVZ2lVYE0"}},{"cell_type":"markdown","source":["Vamos a estimar el alpha y la beta respecto a un índice, el S&P500, usando una regresión lineal mediante descenso por gradiente."],"metadata":{"id":"-fZeWCRk05lr"}},{"cell_type":"code","source":["import yfinance as yf\n","from sklearn.metrics import r2_score, mean_squared_error"],"metadata":{"id":"mixVU2_DVkbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def capm(activo1, activo2, start=\"2022-01-01\", end=\"2023-01-01\"):\n","  # Descarga y preparado de datos\n","  precios = yf.download([activo1, activo2], start=start, end=end)[\"Adj Close\"].dropna()\n","  retornos = np.log(precios).diff().dropna()\n","  Y = retornos[activo1].values\n","  X = retornos[activo2].values\n","\n","  # Modelo de regresión lineal\n","  lr = SGDRegressor(penalty=None, tol=None, learning_rate=\"constant\", max_iter=10000)\n","  lr.fit(X[:, None], Y)\n","  preds = lr.predict(X[:, None])\n","\n","  # Métricas\n","  r2 = r2_score(Y, preds)\n","  mse = mean_squared_error(preds, Y)\n","  betas = lr.coef_\n","  alpha = lr.intercept_\n","\n","  # Muestra de resultados\n","  print(\" > El modelo ha sido entrenado con:\")\n","  print(\"   iteraciones:\", lr.n_iter_)\n","  print(\"   R2 score:\", r2)\n","  print(\"   MSE:\", mse)\n","  print(\"   Beta:\", betas[0])\n","  print(\"   Alpha:\", alpha[0])\n","\n","  plt.figure(figsize=(10, 3))\n","  plt.plot(precios[activo1]/precios[activo1].iloc[0], label=activo1)\n","  plt.plot(precios[activo2]/precios[activo2].iloc[0], label=activo2)\n","  plt.legend()\n","  plt.show()\n","\n","  plt.plot(X, Y, '.')\n","  plt.plot([X.min(), X.max()], [X.min() * betas[0] + alpha[0], X.max() * betas[0] + alpha[0]], '-')\n","  plt.xlabel(activo2)\n","  plt.ylabel(activo1)\n","  plt.show()"],"metadata":{"id":"sXxhozxWd04L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Si comparamos APPLE con el índice, vemos que se puede explicar un 78% del índice (R2 score). Este efecto se ve reflejado en la segunda figura, donde los puntos están más o menos desplegados sobre la recta de la regresión lineal."],"metadata":{"id":"feZCveBr08X-"}},{"cell_type":"code","source":["capm(\"AAPL\", \"SPY\")"],"metadata":{"id":"dGQulcEOeRth"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sin embargo, si comparamos con otro índice (^FVX), vemos que no explica prácticamente nada, solamente un 1%. Este efecto se ve reflejado en la prácticamente nula dispersión sobre la recta."],"metadata":{"id":"I6ndP_pr1AWc"}},{"cell_type":"code","source":["capm(\"^FVX\", \"SPY\")"],"metadata":{"id":"zK-9pedWZfjQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ¿Y si intentamos replicar un índice con una regresión lineal?"],"metadata":{"id":"UbPYhgMg1DLx"}},{"cell_type":"markdown","source":["Si usamos como factores los activos que consideramos importantes, podemos intentar recuperar el índice S&P500 a partir de los activos."],"metadata":{"id":"KZRDLjT21Fbo"}},{"cell_type":"code","source":["def capm_replica(lista_activos, indice, start=\"2022-01-01\", end=\"2023-01-01\", verbose=False):\n","  # Descarga y preparado de datos\n","  precios = yf.download(lista_activos + [indice], start=start, end=end)[\"Adj Close\"].dropna()\n","  retornos = np.log(precios).diff().dropna()\n","  Y = retornos[indice].values\n","  X = retornos[lista_activos].values\n","\n","  # Modelo de regresión lineal\n","  lr = SGDRegressor(penalty=None, tol=None, learning_rate=\"constant\", max_iter=10000)\n","  lr.fit(X, Y)\n","  preds = lr.predict(X)\n","\n","  # Métricas\n","  r2 = r2_score(Y, preds)\n","  mse = mean_squared_error(preds, Y)\n","  betas = lr.coef_\n","  alpha = lr.intercept_\n","\n","  if verbose:\n","    # Muestra de resultados\n","    print(\" > El modelo ha sido entrenado con:\")\n","    print(\"   iteraciones:\", lr.n_iter_)\n","    print(\"   R2 score:\", r2)\n","    print(\"   MSE:\", mse)\n","    #print(\"   Beta:\", betas[0])\n","    #print(\"   Alpha:\", alpha[0])\n","\n","  return betas, retornos, precios[indice]"],"metadata":{"id":"h1IqimQkeUbk","executionInfo":{"status":"ok","timestamp":1715350048876,"user_tz":-120,"elapsed":13,"user":{"displayName":"Christian Oliva","userId":"06496184645047625135"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["Prueba a añadir o quitar algún activo más, a ver si consigues un buen resultado."],"metadata":{"id":"j0_qqdCO1Itn"}},{"cell_type":"code","source":["_ = capm_replica([\"AAPL\", \"META\", \"MSFT\", \"AMZN\", \"GOOGL\", \"JNJ\"], \"SPY\", verbose=True)"],"metadata":{"id":"_b4ryzgo1Hku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Prueba ahora a entrenar una regresión lineal con esta lista de activos."],"metadata":{"id":"LQJEEnK21LQc"}},{"cell_type":"code","source":["lista_activos = ['A', 'AAPL', 'ABBV', 'ABT', 'ACN', 'ADBE', 'ADM', 'ADSK',\n","                 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIZ', 'AKAM', 'ALL', 'ALLE',\n","                 'AMAT', 'AME', 'AMGN', 'AMP', 'AMT', 'AMZN', 'AON', 'APA', 'APD',\n","                 'APH', 'APTV', 'AVB', 'AVGO', 'AVY', 'AXP', 'AZO', 'BA', 'BAC',\n","                 'BALL', 'BAX', 'BBWI', 'BBY', 'BDX', 'BEN', 'BK', 'BKNG',\n","                 'BLK', 'BMY', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH',\n","                 'CAT', 'CB', 'CBRE', 'CCI', 'CCL', 'CF', 'CHRW', 'CI', 'CINF',\n","                 'CL', 'CLX', 'CMA', 'CMCSA', 'CMG', 'CMI', 'CMS', 'CNP', 'COF',\n","                 'COP', 'COST', 'CPB', 'CRM', 'CSCO', 'CTAS', 'CTRA', 'CTSH',\n","                 'CVS', 'CVX', 'D', 'DAL', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR',\n","                 'DIS', 'DLTR', 'DOV', 'DRI', 'DTE', 'DUK', 'DVA', 'EA', 'EBAY',\n","                 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'ELV', 'EMN', 'EMR', 'EOG', 'EQR',\n","                 'EQT', 'ES', 'ESS', 'ETN', 'ETR', 'EW', 'EXC', 'EXPD', 'EXPE', 'F',\n","                 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FI', 'FIS', 'FITB', 'FMC',\n","                 'FSLR', 'GD', 'GE', 'GEN', 'GILD', 'GIS', 'GL', 'GLW', 'GM', 'GOOG',\n","                 'GOOGL', 'GPC', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HD', 'HES', 'HIG',\n","                 'HON', 'HPQ', 'HRL', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IFF', 'INTC',\n","                 'INTU', 'IP', 'IPG', 'IRM', 'ISRG', 'ITW', 'IVZ', 'JCI', 'JNJ',\n","                 'JNPR', 'JPM', 'K', 'KDP', 'KEY', 'KIM', 'KLAC', 'KMB', 'KMI', 'KMX',\n","                 'KO', 'KR', 'L', 'LDOS', 'LEN', 'LH', 'LHX', 'LLY', 'LMT', 'LNC',\n","                 'LOW', 'LRCX', 'LUV', 'LYB', 'MA', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO',\n","                 'MDT', 'MET', 'META', 'MHK', 'MKC', 'MLM', 'MMC', 'MMM', 'MO', 'MOS',\n","                 'MPC', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTB', 'NDAQ', 'NEE', 'NEM',\n","                 'NFLX', 'NI', 'NKE', 'NOC', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE',\n","                 'NVDA', 'NWL', 'NWSA', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PARA',\n","                 'PAYX', 'PCAR', 'PCG', 'PEG', 'PFE', 'PG', 'PGR', 'PH', 'PHM',\n","                 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRU', 'PSA', 'PSX',\n","                 'PTC', 'PWR', 'PXD', 'QCOM', 'RCL', 'RF', 'RHI', 'RL', 'ROK', 'ROP',\n","                 'ROST', 'RSG', 'RTX', 'RVTY', 'SBUX', 'SCHW', 'SEE', 'SHW', 'SJM',\n","                 'SLB', 'SNA', 'SO', 'SPG', 'SPGI', 'SRE', 'STT', 'STX', 'STZ', 'SWK',\n","                 'SYK', 'SYY', 'T', 'TAP', 'TEL', 'TFC', 'TGT', 'TJX', 'TMO', 'TPR',\n","                 'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TXT', 'UHS', 'UNH', 'UNP', 'UPS',\n","                 'URI', 'USB', 'V', 'VFC', 'VLO', 'VMC', 'VRSN', 'VTR', 'VZ', 'WAT',\n","                 'WEC', 'WELL', 'WFC', 'WHR', 'WM', 'WMB', 'WMT', 'WY', 'WYNN', 'XOM',\n","                 'XRAY', 'XYL', 'YUM', 'ZBH', 'ZION', 'ZTS']"],"metadata":{"id":"tY-LOTPS1LoA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Fíjate cómo podemos obtener un R2 muy alto. Estamos explicando un 99.95% del índice."],"metadata":{"id":"hBbJ5csB1NXz"}},{"cell_type":"code","source":["betas, retornos, precios_sp500 = capm_replica(lista_activos, \"SPY\", verbose=True)"],"metadata":{"id":"WMeA5LyG1NvV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sin embargo, estamos considerando betas (pesos) con valor negativo. Un índice no debería tener pesos negativos. Sin embargo, sí podemos hacer una cartera con estos pesos."],"metadata":{"id":"h5K_IiXr1QAb"}},{"cell_type":"code","source":["plt.bar(range(len(betas)), betas)"],"metadata":{"id":"d1hdDIma1ObV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pesos = np.copy(betas)\n","pesos = pesos / pesos.sum()"],"metadata":{"id":"srPVCX0V1SjB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retornos_cartera = retornos.iloc[:, :-1] @ pesos"],"metadata":{"id":"6owVN1vi1Txc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Con ligeras variaciones, hemos conseguido replicar el índice del S&P 500 con pesos negativos."],"metadata":{"id":"aBt36Egq1VCS"}},{"cell_type":"code","source":["plt.plot(precios_sp500/precios_sp500.iloc[0], label=\"SP 500\")\n","plt.plot(np.exp(retornos_cartera.cumsum()), label=\"Cartera\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"rqxnIA3c1Vc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¿Cómo podríamos intentar entrenar un modelo de regresión lineal evitando pesos negativos? Como hemos visto, la regularización L1 nos ayuda a reducir a 0 aquellos pesos innecesarios. Además, como sabemos que el índice está compuesto por activos, todos ellos con pesos positivos, seguramente la regresión lineal sea capaz de detectarlo."],"metadata":{"id":"p9b5qTel1XbS"}},{"cell_type":"code","source":["def capm_replica(lista_activos, indice, start=\"2022-01-01\", end=\"2023-01-01\", verbose=False):\n","  # Descarga y preparado de datos\n","  precios = yf.download(lista_activos + [indice], start=start, end=end)[\"Adj Close\"].dropna()\n","  retornos = np.log(precios).diff().dropna()\n","  Y = retornos[indice]\n","  X = retornos[lista_activos]\n","\n","  # Modelo de regresión lineal\n","  ## NUEVO: Añadimos penalty=\"l1\" y un alpha > 0 para añadir regularización\n","  lr = SGDRegressor(penalty=\"l1\", tol=None, alpha=0.00001, learning_rate=\"constant\", max_iter=10000)\n","  lr.fit(X, Y)\n","  preds = lr.predict(X)\n","\n","  # Métricas\n","  r2 = r2_score(Y, preds)\n","  mse = mean_squared_error(preds, Y)\n","  betas = lr.coef_\n","  alpha = lr.intercept_\n","\n","  if verbose:\n","    # Muestra de resultados\n","    print(\" > El modelo ha sido entrenado con:\")\n","    print(\"   iteraciones:\", lr.n_iter_)\n","    print(\"   R2 score:\", r2)\n","    print(\"   MSE:\", mse)\n","    #print(\"   Betas:\", betas)\n","    #print(\"   Alpha:\", alpha[0])\n","\n","  return betas, retornos, precios[indice]"],"metadata":{"id":"pJXDDbAE1YCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hemos perdido un poco de R2 (99.32% en mi ejecución), pero a cambio hemos conseguido anular los pesos en gran medida."],"metadata":{"id":"-BHrUGyJ1ZDy"}},{"cell_type":"code","source":["betas, retornos, precios_sp500 = capm_replica(lista_activos, \"SPY\", verbose=True)"],"metadata":{"id":"ZvP5VZi51Z9z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.bar(range(len(betas)), betas)"],"metadata":{"id":"dugoTGd61bzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pesos = np.copy(betas)\n","pesos = pesos / pesos.sum()"],"metadata":{"id":"9-y0Ru871cjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["retornos_cartera = retornos.iloc[:, :-1] @ pesos"],"metadata":{"id":"dwVkU4b-1dV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.plot(precios_sp500/precios_sp500.iloc[0], label=\"SP 500\")\n","plt.plot(np.exp(retornos_cartera.cumsum()), label=\"Cartera\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"T5EicYbA1egg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Observaciones finales**\n","\n","- El índice se rebalancea y nosotros no lo estamos haciendo.\n","- ¿Podríamos intentar analizar la composición de un fondo con esta técnica?"],"metadata":{"id":"Fv9VNPNh1bB9"}},{"cell_type":"code","source":[],"metadata":{"id":"qkrsqaI61g0B"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}