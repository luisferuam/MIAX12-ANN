{"cells":[{"cell_type":"markdown","metadata":{"id":"p_uxTWnlyd1Q"},"source":["<img>\n","<font color=\"#CA3532\"><h1 align=\"left\">Deep Learning</h1></font>\n","<font color=\"#6E6E6E\"><h2 align=\"left\">Introducción a Keras</h2></font>"]},{"cell_type":"markdown","metadata":{"id":"BYPdXYszyd1S"},"source":["### <font color=\"#CA3532\">Recursos</font>\n","\n","- Página oficial: https://keras.io/\n","- Getting started with the Keras Sequential model: https://keras.io/getting-started/sequential-model-guide/\n","- Keras guide (de la página de TensorFlow): https://www.tensorflow.org/guide/keras\n","- Libro de Francois Chollet, *Deep Learning with Python*: https://www.manning.com/books/deep-learning-with-python"]},{"cell_type":"markdown","metadata":{"id":"k9qWoPVLyd1T"},"source":["### <font color=\"#CA3532\">Resolviendo MNIST con Keras</font>\n","\n","En este notebook vamos a construir una red neuronal para el problema MNIST (http://yann.lecun.com/exdb/mnist/) usando Keras. Lo primero, como siempre, es importar las librerías necesarias:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxL8U9Jdyd1U","scrolled":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from time import time\n","import shutil"]},{"cell_type":"markdown","metadata":{"id":"CdvFKkKvyd1Z"},"source":["Cargamos los datos de MNIST:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uY4N7AEZyd1a"},"outputs":[],"source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","print(train_images.shape)\n","print(train_labels.shape)\n","print(train_labels)\n","\n","print(test_images.shape)\n","print(test_labels.shape)\n","print(test_labels)"]},{"cell_type":"markdown","metadata":{"id":"ijpLDkmTyd1g"},"source":["Dibujamos algunas de las imágenes:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmCTexh5yd1h"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(train_images[i], cmap=plt.cm.binary)\n","    plt.xlabel(train_labels[i])"]},{"cell_type":"markdown","metadata":{"id":"WKqwpZG8yd1l"},"source":["Antes de construir los modelos normalizamos las imágenes dividiendo entre el valor máximo para tenerlo entre 0 y 1:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EFR_Um5Vyd1m"},"outputs":[],"source":["train_images = train_images / 255\n","test_images = test_images / 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFA6RI2_ntBt"},"outputs":[],"source":["plt.imshow(train_images[0], cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"O31bdxH3ng3v"},"source":["Ahora restamos la media:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhqefaaQndAy"},"outputs":[],"source":["mean_img = train_images.mean(axis=0)\n","train_images = train_images - mean_img\n","test_images = test_images - mean_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ax-Jtw2N6SX2"},"outputs":[],"source":["plt.imshow(mean_img, cmap=plt.cm.binary)\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uhg59kHm6gFt"},"outputs":[],"source":["plt.imshow(train_images[0], plt.cm.binary, vmin=-1, vmax=1)\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"NhSkIfb-yd1p"},"source":["Vamos a crear una red muy sencilla para este problema. Usaremos una única capa oculta con **30 unidades sigmoides**. La activación en la salida será **SoftMax** y como función de coste usaremos **cross-entropy**.\n","\n","En Keras hay que definir el modelo como un conjunto de capas apiladas unas sobre otras de manera secuencial. La clase básica para implementar un modelo es <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential\"> tf.keras.Sequential</a>."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HT9Vd6E2yd1q"},"outputs":[],"source":["model = keras.Sequential()"]},{"cell_type":"markdown","metadata":{"id":"4uyIhj8Nyd1t"},"source":["Sobre este modelo vamos apilando las capas. La primera de nuestro modelo será una capa que \"aplane\" la entrada, transformando las imágenes de 28x28 píxeles a vectores con 784 componentes. Nótese que, por estar la primera capa directamente conectada a la entrada, es necesario especificar el tamaño de la misma con el argumento *input_shape*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5m4hziN6yd1u"},"outputs":[],"source":["model.add(keras.layers.Flatten(input_shape=(28, 28), name=\"entrada\"))"]},{"cell_type":"markdown","metadata":{"id":"udg68poZyd1x"},"source":["A continuación añadimos la capa oculta con 30 unidades sigmoides:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gw1zlRJryd1z"},"outputs":[],"source":["model.add(keras.layers.Dense(30, activation=\"sigmoid\", name=\"oculta\"))"]},{"cell_type":"markdown","metadata":{"id":"ppct8hPfyd15"},"source":["Y finalmente añadimos la capa de salida con 10 unidades de tipo *softmax*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKrbEvk-yd16"},"outputs":[],"source":["model.add(keras.layers.Dense(10, activation=\"softmax\", name=\"salida\"))"]},{"cell_type":"markdown","metadata":{"id":"PhxprszLyd1-"},"source":["El método *summary()* nos muestra un resumen del modelo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKxsFzKz6g8x"},"outputs":[],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"id":"W_dT-1IcYE3_"},"source":["También podemos mostrar el resumen como un grafo de conexiones entre capas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QedBIdIFYCMK"},"outputs":[],"source":["tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"1WSJbhr6yd2C"},"source":["Una vez hemos construido el modelo, es necesario decirle a Keras cómo vamos a entrenarlo. Para ello tenemos que llamar al método *compile()*, indicando el optimizador, la función de coste y las métricas que vamos a usar para evaluar el modelo. En este caso usaremos un optimizador de descenso por gradiente estándar, la función de coste cross-entropy y la métrica *accuracy*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mYLFmeZByd2D"},"outputs":[],"source":["model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['acc'])"]},{"cell_type":"markdown","metadata":{"id":"W4XMfG4tyd2F"},"source":["Y finalmente entrenamos el modelo invocando al método *fit()*. Se pueden especificar, entre otros, los siguientes argumentos:\n","\n","- Los datos de entrenamiento junto con sus etiquetas correspondientes.\n","- El número de épocas (10 en el ejemplo).\n","- El tamaño del batch (1000 en el ejemplo).\n","- Los datos de validación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Txx6gSlRyd2G"},"outputs":[],"source":["nepochs = 50\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=nepochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=1000)"]},{"cell_type":"markdown","metadata":{"id":"btPN2lRDyd2J"},"source":["El método *fit()* devuelve un objeto de la clase *History*. En este, el atributo *history* contiene el valor del coste y las métricas de evaluación en cada época para el conjunto de entrenamiento. Si se han usado datos de validación, el coste y las métricas también están disponibles para éste. Podemos usar estos datos para representar gráficamente la evolución del coste y la precisión:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9LbJd8Rxyd2K"},"outputs":[],"source":["hd = history.history\n","\n","epochs = range(1, nepochs+1)\n","\n","plt.figure(figsize=(12,6))\n","\n","plt.subplot(1,2,1)\n","plt.plot(epochs, hd['acc'], \"r\", label=\"train\")\n","plt.plot(epochs, hd['val_acc'], \"b\", label=\"valid\")\n","plt.grid(True)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"accuracy\")\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(epochs, hd['loss'], \"r\", label=\"train\")\n","plt.plot(epochs, hd['val_loss'], \"b\", label=\"valid\")\n","plt.grid(True)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.title(\"Loss\")\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RflRGJv4yd2M"},"source":["### <font color=\"#CA3532\">TensorBoard</font>\n","\n","TensorBoard (https://www.tensorflow.org/tensorboard ) es una herramienta de visualización que nos permite, entre otras cosas, monitorizar el entrenamiento de la red. Para usar TensorBoard desde Google Colab hacemos lo siguiente:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GdZDx9kb9wbU"},"outputs":[],"source":["## NOTA: En algunas versiones de Firefox no funciona. Se recomienda utilizar Google Chrome"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"shnmcZv3Kcin"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOF4U1tmAjIa"},"outputs":[],"source":["%tensorboard --logdir logs"]},{"cell_type":"markdown","metadata":{"id":"gON6Umnw9Kdq"},"source":["Esto lanza la herramienta TensorBoard dentro de colab. Hemos especificado un directorio de logs del que TensorBoard leerá la información. Tenemos que decirle a keras que guarde la información de cada época en el mismo directorio, y esto lo hacemos mediante un *callback*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PzDii8BNyd2N"},"outputs":[],"source":["# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=(28, 28), name=\"entrada\"))\n","model.add(keras.layers.Dense(30, activation=\"sigmoid\", name=\"oculta\"))\n","model.add(keras.layers.Dense(10, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=\"logs/prueba-1\", histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=100,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=1000,\n","                    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"yDFDTE99yd2S"},"source":["### <font color=\"#CA3532\">Evaluación del modelo y predicciones</font>\n","\n","Una vez entrenado el modelo, podemos llamar a *evaluate()* para evaluarlo sobre un conjunto de datos (test) o a *predict()* para obtener sus predicciones.\n","\n","El método *evaluate()* obtiene las métricas de evaluación sobre un conjunto de datos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4j9SKyCyd2S"},"outputs":[],"source":["loss_test, acc_test = model.evaluate(test_images, test_labels)\n","print(\"Loss on test set = %f\" % (loss_test))\n","print(\"Accuracy on test set = %f\" % (acc_test))"]},{"cell_type":"markdown","metadata":{"id":"pBasGfwryd2V"},"source":["El método *predict()* devuelve las predicciones del modelo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KSxOcu2yd2W"},"outputs":[],"source":["predictions = model.predict(test_images)\n","print(predictions.shape)"]},{"cell_type":"markdown","metadata":{"id":"p1qRHpknyd2Z"},"source":["Podemos comparar las predicciones con las clases reales para obtener un accuracy que debe ser igual que el anterior:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28sS_UDTyd2b"},"outputs":[],"source":["y_test = np.argmax(predictions, axis=1)\n","aciertos = y_test == test_labels\n","acc_v2 = np.mean(aciertos)\n","print(\"Accuracy on test set (v2)= %f\" % (acc_v2))"]},{"cell_type":"markdown","metadata":{"id":"8fmrfhfPkVyt"},"source":["Veamos qué tal lo hace la red:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ON-ZwXmmo1KU"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","for i in range(25):\n","    plt.subplot(5,5,i+1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.grid(False)\n","    plt.imshow(test_images[i], cmap=plt.cm.binary)\n","    plt.xlabel(\"%d, %d\" % (test_labels[i], y_test[i]))"]},{"cell_type":"markdown","metadata":{"id":"0sU8LKTrHnB_"},"source":["### Matriz de confusión:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AXilVjr0Hq_f"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","\n","confusion_matrix(test_labels, y_test)"]},{"cell_type":"markdown","metadata":{"id":"lp836-_9yd2h"},"source":["### <font color=\"#CA3532\">Cómo guardar un modelo entrenado</font>\n","\n","Para guardar los pesos del modelo una vez entrenado, usamos el método *save_weights()*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w459AtVAyd2h"},"outputs":[],"source":["model.save_weights('./logs/keras/modelo')\n","!ls logs/keras"]},{"cell_type":"markdown","metadata":{"id":"E_kNGcWAyd2k"},"source":["Para cargar los pesos posteriormente, usamos el método *load_weights*. Para poder cargar los pesos en un modelo, el modelo debe estar creado de la misma manera que los pesos guardados:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wstkEtCnyd2k"},"outputs":[],"source":["# Creamos un nuevo modelo con la misma estructura que el anterior:\n","model2 = keras.Sequential()\n","model2.add(keras.layers.Flatten(input_shape=(28, 28)))\n","model2.add(keras.layers.Dense(30, activation=\"sigmoid\"))\n","model2.add(keras.layers.Dense(10, activation=\"softmax\"))\n","\n","# Compilamos el modelo nuevo:\n","model2.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['acc'])\n","\n","# Cargamos los pesos a partir del fichero anterior:\n","model2.load_weights('./logs/keras/modelo')\n","\n","# Evaluamos el modelo sobre el conjunto de test:\n","loss_test, acc_test = model2.evaluate(test_images, test_labels)\n","print(\"Loss on test set = %f\" % (loss_test))\n","print(\"Accuracy on test set = %f\" % (acc_test))"]},{"cell_type":"markdown","metadata":{"id":"6ei7dsxEyd2n"},"source":["También es posible guardar un modelo completo, incluyendo la arquitectura y los pesos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HeiBf_0Wyd2p"},"outputs":[],"source":["model.save('./logs/keras/modelo.h5')"]},{"cell_type":"markdown","metadata":{"id":"u76zZ1zfyd2s"},"source":["Para cargar el modelo completo, usamos *keras.models.load_model()*:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z5DBdVGbyd2t"},"outputs":[],"source":["model3 = keras.models.load_model('./logs/keras/modelo.h5')\n","model3.summary()\n","loss_test, acc_test = model.evaluate(test_images, test_labels)\n","print(\"Loss on test set = %f\" % (loss_test))\n","print(\"Accuracy on test set = %f\" % (acc_test))"]},{"cell_type":"markdown","metadata":{"id":"N89gSL5Byd2v"},"source":["### <font color=\"#CA3532\">Algunos ejemplos de capas disponibles en Keras</font>"]},{"cell_type":"markdown","metadata":{"id":"0edeTSxTyd2w"},"source":["Capa completamente conectada de 30 unidades ReLU:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ElHSUFJsyd2x"},"outputs":[],"source":["keras.layers.Dense(30, activation=\"relu\")"]},{"cell_type":"markdown","metadata":{"id":"MJbTw2TAyd20"},"source":["Que también se puede especificar así:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMYQcp3Ryd20"},"outputs":[],"source":["keras.layers.Dense(30, activation=tf.nn.relu)"]},{"cell_type":"markdown","metadata":{"id":"iMDfSl8Lyd23"},"source":["Capa completamente conectada de 30 unidades sigmoides:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1R6bBs9Nyd24"},"outputs":[],"source":["keras.layers.Dense(30, activation=\"sigmoid\")"]},{"cell_type":"markdown","metadata":{"id":"dtiTVKCiyd27"},"source":["Que también se puede especificar así:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsxo0CEZyd27"},"outputs":[],"source":["keras.layers.Dense(30, activation=tf.nn.sigmoid)"]},{"cell_type":"markdown","metadata":{"id":"Y99YhN1dyd2-"},"source":["Capa lineal (no se especifica función de activación) con regularización L1 aplicada a los pesos:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3Zrza0uyd2_"},"outputs":[],"source":["keras.layers.Dense(30, kernel_regularizer=keras.regularizers.l1(0.01))"]},{"cell_type":"markdown","metadata":{"id":"8kGYhD8myd3B"},"source":["Capa lineal (no se especifica función de activación) con regularización L2 aplicada al bias:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RtSqGPcKyd3B"},"outputs":[],"source":["keras.layers.Dense(30, bias_regularizer=keras.regularizers.l2(0.01))"]},{"cell_type":"markdown","metadata":{"id":"IwBCuto_yd3G"},"source":["Capa lineal (no se especifica función de activación) con los pesos inicializados según una normal N(0, 0.1):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iCsWDXoSyd3H"},"outputs":[],"source":["keras.layers.Dense(30, kernel_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.1))"]},{"cell_type":"markdown","metadata":{"id":"K4_ZpHk-yd3P"},"source":["Capa lineal (no se especifica función de activación) con los pesos inicializados según Xavier:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fi-WUjwryd3R"},"outputs":[],"source":["keras.layers.Dense(30, kernel_initializer=keras.initializers.glorot_normal())"]},{"cell_type":"markdown","metadata":{"id":"pSPqPXcMyd3a"},"source":["Capa que sólo aplica la función de activación (ReLU):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WtAowLjyd3b"},"outputs":[],"source":["keras.layers.Activation(tf.nn.relu)"]},{"cell_type":"markdown","metadata":{"id":"Tt7dffNqyd3f"},"source":["Capa de dropout con una probabilidad de 0.2 de eliminar una neurona:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DFgULJXgyd3i"},"outputs":[],"source":["keras.layers.Dropout(0.2)"]},{"cell_type":"markdown","metadata":{"id":"cB02ipfFyd3m"},"source":["Capa de normalización batch:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xrx2rS_Yyd3n"},"outputs":[],"source":["keras.layers.BatchNormalization()"]},{"cell_type":"markdown","metadata":{"id":"1P96G8gZyd3t"},"source":["El siguiente modelo combina algunas de las capas anteriores:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXrRpFzVyd3x"},"outputs":[],"source":["model = keras.Sequential()\n","\n","model.add(keras.layers.Flatten(input_shape=(28, 28)))\n","model.add(keras.layers.Dense(256, kernel_regularizer=keras.regularizers.l2(1.0)))\n","model.add(keras.layers.BatchNormalization())\n","model.add(keras.layers.Activation(tf.nn.relu))\n","model.add(keras.layers.Dropout(rate=0.5))\n","model.add(keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l2(1.0)))\n","model.add(keras.layers.BatchNormalization())\n","model.add(keras.layers.Activation(tf.nn.relu))\n","model.add(keras.layers.Dense(10, kernel_regularizer=keras.regularizers.l2(1.0)))\n","model.add(keras.layers.Activation(tf.nn.softmax))"]},{"cell_type":"markdown","metadata":{"id":"oOIvj2s5yd33"},"source":["Imprimamos un resumen del modelo:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0TMNBOxyd36"},"outputs":[],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_cND78SZyd4A"},"outputs":[],"source":["model.compile(optimizer=keras.optimizers.Adam(),\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['acc'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_cEQaNwyd4F"},"outputs":[],"source":["nepochs = 30\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=nepochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=1000)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjX7rgsGyd4K"},"outputs":[],"source":["hd = history.history\n","\n","epochs = range(1, nepochs+1)\n","\n","plt.figure(figsize=(12,6))\n","\n","plt.subplot(1,2,1)\n","plt.plot(epochs, hd['acc'], \"r\", label=\"train\")\n","plt.plot(epochs, hd['val_acc'], \"b\", label=\"valid\")\n","plt.grid(True)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"accuracy\")\n","plt.title(\"Accuracy\")\n","plt.legend()\n","\n","plt.subplot(1,2,2)\n","plt.plot(epochs, hd['loss'], \"r\", label=\"train\")\n","plt.plot(epochs, hd['val_loss'], \"b\", label=\"valid\")\n","plt.grid(True)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")\n","plt.title(\"Loss\")\n","plt.legend()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-Q8KpxLzyd4O"},"source":["## <font color=\"#CA3532\">Ejercicio</color>\n","\n","Entrenar un buen modelo para MNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MLsgMnKvp1S"},"outputs":[],"source":["# Variables que no vamos a modificar\n","log_dir = \"./models/\"\n","input_shape = (28, 28)\n","num_clases = 10\n","n_epochs = 10\n","\n","LEARNING_RATE_BASE = 0.01\n","LEARNING_RATE_SMALL = 0.0001\n","LEARNING_RATE_BIG = 1.0\n","\n","BATCH_SIZE_BASE = 400\n","BATCH_SIZE_BIG = 1000\n","BATCH_SIZE_SMALL = 50"]},{"cell_type":"markdown","metadata":{"id":"gLBQDPL1rBw3"},"source":["### <font color=\"#CA3532\">Modelo base</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"raDBi04vm2zG"},"outputs":[],"source":["# Caso base:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'base'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VoPRclqBrDNW"},"outputs":[],"source":["# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"dpbG2hm__KmN"},"source":["### <font color=\"#CA3532\">Prueba de diferentes learning-rate</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_tv3RbC_QEs"},"outputs":[],"source":["# Prueba learning-rate small:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_SMALL\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'learningRate-small'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl7jcdcE_aAj"},"outputs":[],"source":["# Prueba learning-rate big:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8lAWtZzPW34"},"outputs":[],"source":["# Prueba learning-rate verybig:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BIG * 10000\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'learningRate-verybig'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U051u8MJ_f__"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"F7wmVbz4Svg6"},"source":["### <font color=\"#CA3532\">Prueba de diferentes batch-size</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQZ32BQOw_cM"},"outputs":[],"source":["# Caso batchsize big:\n","batch_size = BATCH_SIZE_BIG\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-yeThSdWdLJ4"},"outputs":[],"source":["# Caso batchsize small:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-small'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECNj3ZeT73X5"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"O-RrjQ5b_Dnj"},"source":["### <font color=\"#CA3532\">Prueba de diferentes learning-rate con diferentes batch-size</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CKaRhiRt77At"},"outputs":[],"source":["# Caso batchsize small learningrate small:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_SMALL\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-small-learningRate-small'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2L6kZmvP77DC"},"outputs":[],"source":["# Caso batchsize small learningrate big:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVGAZGn4lnPm"},"outputs":[],"source":["# Caso batchsize small learningrate verybig:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG * 10000\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-small-learningRate-verybig'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29ZPg6QE77Fa"},"outputs":[],"source":["# Caso batchsize big learningrate small:\n","batch_size = BATCH_SIZE_BIG\n","learning_rate = LEARNING_RATE_SMALL\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-big-learningRate-small'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6PrrYU-nFg5i"},"outputs":[],"source":["# Caso batchsize big learningrate big:\n","batch_size = BATCH_SIZE_BIG\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-big-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PfmbELVslG1w"},"outputs":[],"source":["# Caso batchsize big learningrate verybig:\n","batch_size = BATCH_SIZE_BIG\n","learning_rate = LEARNING_RATE_BIG * 10000\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'batchSize-big-learningRate-verybig'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kkU0M_dP73cn"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"Jx7QuzJwF9rN"},"source":["### <font color=\"#CA3532\">Prueba con función de coste diferentes</font>"]},{"cell_type":"markdown","metadata":{"id":"29W1QBgso-N1"},"source":["Para utilizar las funciones de coste ``mean_squared_error`` y ``categorical_hinge``, es necesario que los labels estén en formato onehot (también conocido como labels categóricos). Podéis consultar la API de Keras:\n","\n","**Todos los losses disponibles**: https://keras.io/api/losses/\n","\n","**MSE**: https://keras.io/api/losses/regression_losses/#mean_squared_error-function\n","\n","**HINGE**: https://keras.io/api/losses/hinge_losses/#categorical_hinge-function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdKapBIkp23g"},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqadbmJSi3o0"},"outputs":[],"source":["train_labels_categorical = to_categorical(train_labels)\n","test_labels_categorical = to_categorical(test_labels)\n","print(train_labels[:10])\n","print(train_labels_categorical[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TnkE4pvnDyHi"},"outputs":[],"source":["# Caso MSE:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'sigmoid'\n","loss = 'mean_squared_error' ### IMPORTANTE cambiar aquí mean_squared_error\n","nombre = 'mse'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels_categorical, ### IMPORTANTE cambiar aquí train_labels_categorical\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels_categorical), ### IMPORTANTE cambiar aquí test_labels_categorical\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cS_SDa2nqaaH"},"outputs":[],"source":["# Caso MSE con learning rate BIG:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'mean_squared_error' ### IMPORTANTE cambiar aquí mean_squared_error\n","nombre = 'mse-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels_categorical, ### IMPORTANTE cambiar aquí train_labels_categorical\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels_categorical), ### IMPORTANTE cambiar aquí test_labels_categorical\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n7NLO9R8DyJ0"},"outputs":[],"source":["# Caso hinge:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'sigmoid'\n","loss = 'categorical_hinge' ### IMPORTANTE cambiar aquí categorical_hinge\n","nombre = 'hinge'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=None, name=\"salida\")) ## IMPORTANTE activation = None\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels_categorical, ### IMPORTANTE cambiar aquí train_labels_categorical\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels_categorical), ### IMPORTANTE cambiar aquí test_labels_categorical\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGv1BTQdrIbm"},"outputs":[],"source":["# Caso hinge con learning rate BIG:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'categorical_hinge' ### IMPORTANTE cambiar aquí categorical_hinge\n","nombre = 'hinge-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=None, name=\"salida\")) ## IMPORTANTE activation = None\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels_categorical, ### IMPORTANTE cambiar aquí train_labels_categorical\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels_categorical), ### IMPORTANTE cambiar aquí test_labels_categorical\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmIfdl4HDyQh"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"6JtjeATOLf_k"},"source":["### <font color=\"#CA3532\">Prueba con diferentes funciones de activación en la capa oculta</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LhMUSHNgDySp"},"outputs":[],"source":["# Caso tanh:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'tanh'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'tanh'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhuGFiDyK0qS"},"outputs":[],"source":["# Caso relu:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'relu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'relu'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZnfdwQ_K0sp"},"outputs":[],"source":["# Caso softplus:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'softplus'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'softplus'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RovzrAfIK0u3"},"outputs":[],"source":["# Caso elu:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'elu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'elu'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F0_Bx3qaK0wI"},"outputs":[],"source":["# Caso selu:\n","batch_size = BATCH_SIZE_BASE\n","learning_rate = LEARNING_RATE_BASE\n","activation = 'selu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'selu'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sZIrBKjVNHpf"},"outputs":[],"source":["# Caso tanh best batch (SMALL) and learningrate (BIG):\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'tanh'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'tanh-batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6rRwfxNNadJ"},"outputs":[],"source":["# Caso relu best batch (SMALL) and learningrate (BIG):\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'relu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'relu-batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IjW9_nAQNpw6"},"outputs":[],"source":["# Caso softplus best batch (SMALL) and learningrate (BIG):\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'softplus'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'softplus-batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MD8-d7cmNu0b"},"outputs":[],"source":["# Caso elu best batch (SMALL) and learningrate (BIG):\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'elu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'elu-batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TGUZx1I1NzZ5"},"outputs":[],"source":["# Caso selu best batch (SMALL) and learningrate (BIG):\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'selu'\n","loss = 'sparse_categorical_crossentropy'\n","nombre = 'selu-batchSize-small-learningRate-big'\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LV6rnNxUK0xN"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"rOGot9PQWe6K"},"source":["### <font color=\"#CA3532\">Prueba cómo contrarrestar Overfitting</font>"]},{"cell_type":"markdown","metadata":{"id":"i_lvjuVDW93Z"},"source":["¿Cómo podemos evitar el Overfitting?\n","\n","- Usando más datos en entrenamiento.\n","\n","- Usando un conjunto de validación y **early stopping**.\n","\n","- Aplicando algún tipo de **regularización** (L1, L2 o Dropout)."]},{"cell_type":"markdown","metadata":{"id":"Do1cdq2WWoGS"},"source":["#### Regularizacion L1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uDeaIQs8z7D2"},"outputs":[],"source":["# Vuelvo a ejecutar el modelo base para un experimento nuevo. Quiero hacer la suma\n","# del valor absoluto de los pesos de la capa densa intermedia. Además, quiero mostrar\n","# los pesos de la capa de entrada de forma global y en particular para cada neurona\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l1reg = 0.0 # Si añades regularizacion 0.0 es como no añadir nada\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l1(l1reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nwr32dgr0nuc"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WQRUhp8DKjzJ"},"outputs":[],"source":["print(model.layers[1].weights[0].shape)\n","plt.imshow(np.abs(model.layers[1].weights[0]).sum(axis=1).reshape(28,-1))\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xG_t-vnMS6YO"},"outputs":[],"source":["max_value = np.abs(model.layers[1].weights[0].numpy()).max()\n","plt.figure(figsize=(15,15))\n","for i, neuron_weights in enumerate(model.layers[1].weights[0].numpy().T):\n","  plt.subplot(8,8,i+1)\n","  plt.title(\"Neurona \"+str(i))\n","  plt.imshow(neuron_weights.reshape(28,28), vmin=-max_value, vmax=max_value, cmap=\"bwr\")\n","  plt.xticks([], [])\n","  plt.yticks([], [])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JvjkdQwXzrAL"},"outputs":[],"source":["# Ahora probamos a añadir la regularización para comparar. Vamos a analizar los pesos\n","# de la misma manera\n","\n","# Caso base con regularización L1 pequeña:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l1reg = 0.0001\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-l1reg-small'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l1(l1reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KhcL055a1oHi"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vL32cl0aODhY"},"outputs":[],"source":["print(model.layers[1].weights[0].shape)\n","plt.imshow(np.abs(model.layers[1].weights[0]).sum(axis=1).reshape(28,-1))\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BrPpw7uDVp91"},"outputs":[],"source":["max_value = np.abs(model.layers[1].weights[0].numpy()).max()\n","plt.figure(figsize=(15,15))\n","for i, neuron_weights in enumerate(model.layers[1].weights[0].numpy().T):\n","  plt.subplot(8,8,i+1)\n","  plt.title(\"Neurona \"+str(i))\n","  plt.imshow(neuron_weights.reshape(28,28), vmin=-max_value, vmax=max_value, cmap=\"bwr\")\n","  plt.xticks([], [])\n","  plt.yticks([], [])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tV_zk0FT2oOO"},"outputs":[],"source":["# Probamos una regularización más alta y volvemos a hacer el análisis de pesos:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l1reg = 0.1\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-l1reg-big'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l1(l1reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QN1vVRaS2vKy"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1APKqwbO0je"},"outputs":[],"source":["print(model.layers[1].weights[0].shape)\n","plt.imshow(np.abs(model.layers[1].weights[0]).sum(axis=1).reshape(28,-1))\n","plt.colorbar()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kQ8kFogQX8R1"},"outputs":[],"source":["max_value = np.abs(model.layers[1].weights[0].numpy()).max()\n","plt.figure(figsize=(15,15))\n","for i, neuron_weights in enumerate(model.layers[1].weights[0].numpy().T):\n","  plt.subplot(8,8,i+1)\n","  plt.title(\"Neurona \"+str(i))\n","  plt.imshow(neuron_weights.reshape(28,28), vmin=-max_value, vmax=max_value, cmap=\"bwr\")\n","  plt.xticks([], [])\n","  plt.yticks([], [])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"5ZkE9ggBYpxP"},"source":["**Discutir los resultados**\n","\n","* ¿Qué está pasando con la regularización L1?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DRNVdh50daLw"},"outputs":[],"source":["# Probamos ahora la activación RELU que sí que tiene más overfitting para ver si las curvas\n","# de aprendizaje ya no se diferencian tanto en train y test\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'relu'\n","loss = 'sparse_categorical_crossentropy'\n","l1reg = 0.001\n","nombre = 'OVERFITTING-relu-batchSize-small-learningRate-big-l1reg-mid'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l1(l1reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"markdown","metadata":{"id":"kajZWQNXcnqO"},"source":["**¿Qué está pasando con L1?**\n","\n","La penalización por regularización L1 ignora el valor de los pesos y provoca un decaimiento de los pesos de manera constante. Un learning rate alto hace que los pesos se penalicen de forma agresiva."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsmT7Skf3bc8"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"ty4c_IteWqo9"},"source":["#### Regularizacion L2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtirAii9457K"},"outputs":[],"source":["# Probamos ahora a introducir L2 en vez de L1 para regularizar\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l2reg = 0.001\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-l2reg-small'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l2(l2reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLMdTpEb5XBQ"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tgznv5bY5c9h"},"outputs":[],"source":["# Probamos un poco más alta la regularización\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l2reg = 0.01\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-l2reg-mid'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l2(l2reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Y8ymPCp5fRQ"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"markdown","metadata":{"id":"QMR_374vZdet"},"source":["**¿Qué está pasando con L2?**\n","\n","La penalización por regularización L2 suaviza la penalización de los pesos, ya que cuanto mayor es el peso mayor es la penalización."]},{"cell_type":"markdown","metadata":{"id":"w8AJnli0bDI0"},"source":["#### Regularizacion L1 + L2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FKzZYQPs5_tc"},"outputs":[],"source":["# Probamos a añadir regularización L1 y L2 a la vez con tf.keras.regularizers.l1_l2\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","l1reg = 0.0001\n","l2reg = 0.01\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-l1reg-small-l2reg-mid'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\",\n","                             kernel_regularizer=keras.regularizers.l1_l2(l1reg, l2reg)))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TS2po2I96HGd"},"outputs":[],"source":["print(model.layers)\n","print(model.layers[1])\n","print(\"\\nSuma de valores absolutos de pesos:\", np.abs(model.layers[1].weights[0]).sum())"]},{"cell_type":"markdown","metadata":{"id":"lsgkdx6gWs5C"},"source":["#### Dropout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85qCLvya6SGQ"},"outputs":[],"source":["# Vamos a probar añadir dropout entre la capa flatten y la capa densa intermedia\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","dropout = 0.2\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-dropout-02'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dropout(dropout))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kHHpJflW7W6W"},"outputs":[],"source":["# Vamos a probar añadir dropout entre la capa flatten y la capa densa intermedia\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","dropout = 0.5\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-dropout-05'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dropout(dropout))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rnUHMhau7nIq"},"outputs":[],"source":["# Vamos a probar añadir dropout entre la capa flatten y la capa densa intermedia\n","\n","# Caso base:\n","batch_size = BATCH_SIZE_SMALL\n","learning_rate = LEARNING_RATE_BIG\n","activation = 'sigmoid'\n","loss = 'sparse_categorical_crossentropy'\n","dropout = 0.8\n","nombre = 'OVERFITTING-base-batchSize-small-learningRate-big-dropout-08'\n","\n","\n","# Volvemos a crear el modelo para que se empiece a entrenar desde 0:\n","model = keras.Sequential()\n","model.add(keras.layers.Flatten(input_shape=input_shape, name=\"entrada\"))\n","model.add(keras.layers.Dropout(dropout))\n","model.add(keras.layers.Dense(64, activation=activation, name=\"oculta\"))\n","model.add(keras.layers.Dense(num_clases, activation=\"softmax\", name=\"salida\"))\n","\n","model.compile(optimizer=keras.optimizers.SGD(learning_rate=learning_rate),\n","              loss=loss,\n","              metrics=['acc'])\n","\n","# Callback a TensorBoard:\n","callbacks = [keras.callbacks.TensorBoard(log_dir=log_dir+\"prueba-\"+nombre, histogram_freq=1, write_images=True)]\n","\n","# Entrenamiento del modelo:\n","history = model.fit(train_images,\n","                    train_labels,\n","                    epochs=n_epochs,\n","                    validation_data=(test_images, test_labels),\n","                    batch_size=batch_size,\n","                    callbacks=callbacks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uV1-45Zd8BtM"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir $log_dir"]},{"cell_type":"markdown","metadata":{"id":"r7DtXKmKaNJL"},"source":["### Ejercicio para casa:\n","\n","¿Qué pasaría si añadimos regularización a la capa de salida? Pruébalo y haz un análisis similar de los pesos. Discutamos los resultados el próximo día."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uw6UOJ589ADd"},"outputs":[],"source":[]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}