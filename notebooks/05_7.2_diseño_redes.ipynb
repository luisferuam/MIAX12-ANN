{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPQcnEHKqgdeCFzUqKy69Mi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Diseño de arquitecturas con Keras"],"metadata":{"id":"-sb-rfALz-cQ"}},{"cell_type":"markdown","source":["En este notebook vamos a realizar algunos ejercicios para trabajar en el diseño de arquitecturas complejas con Keras. Vamos a trabajar sobre el dataset de MNIST."],"metadata":{"id":"IFyXNZ530DE_"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cw7AiQrgjXH_"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import tensorflow as tf"]},{"cell_type":"code","source":["(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n","\n","print(train_images.shape)\n","print(train_labels.shape)\n","print(train_labels)\n","\n","print(test_images.shape)\n","print(test_labels.shape)\n","print(test_labels)"],"metadata":{"id":"2Mir8HuroF4H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_images = train_images / 255\n","test_images = test_images / 255\n","\n","means = train_images.mean(axis=0)\n","train_images = train_images - means\n","test_images = test_images - means"],"metadata":{"id":"qizlpnXdoGqZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio 1"],"metadata":{"id":"y9E8cRtj0bTb"}},{"cell_type":"markdown","source":["El objetivo de este ejercicio consiste en diseñar una red que reciba dos imágenes y te diga si son de la misma clase o no. Para ello, vamos a trabajar con el concepto de similitud, asumiendo que dos imágenes pertenecen a la misma clase si son similares.\n","\n","En primer lugar, y para simplificar el problema, vamos a trabajar exclusivamente con los dígitos 0 y 1."],"metadata":{"id":"MhORQsNB0fTa"}},{"cell_type":"code","source":["# Construimos nuestro dataset en crudo con todas las imágenes de clase 0 y de clase 1\n","\n","dataset_x_train_raw = np.concatenate((train_images[train_labels == 1], train_images[train_labels == 0]))\n","dataset_y_train_raw = np.concatenate((train_labels[train_labels == 1], train_labels[train_labels == 0]))\n","\n","dataset_x_test_raw = np.concatenate((test_images[test_labels == 1], test_images[test_labels == 0]))\n","dataset_y_test_raw = np.concatenate((test_labels[test_labels == 1], test_labels[test_labels == 0]))"],"metadata":{"id":"siEhMHB7pmMI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# En esta celda estamos construyendo el dataset nuevo. El procedimiento es el siguiente:\n","#  1. Selección aleatoria de dos imágenes de entrenamiento: imagen1 y imagen2\n","#  2. Guardamos imagen1 == imagen2 como etiqueta\n","\n","dataset_x_train = []\n","dataset_y_train = []\n","train_size = 50000\n","for i in range(train_size):\n","  i1 = np.random.choice(len(dataset_x_train_raw))\n","  i2 = np.random.choice(len(dataset_x_train_raw))\n","  dataset_x_train.append([dataset_x_train_raw[i1].reshape(28*28), dataset_x_train_raw[i2].reshape(28*28)])\n","  dataset_y_train.append(dataset_y_train_raw[i1] == dataset_y_train_raw[i2])\n","\n","dataset_x_train = np.array(dataset_x_train)\n","dataset_y_train = np.array(dataset_y_train)\n","\n","# Mismo procedimiento para el test\n","\n","dataset_x_test = []\n","dataset_y_test = []\n","test_size = 20000\n","for i in range(test_size):\n","  i1 = np.random.choice(len(dataset_x_test_raw))\n","  i2 = np.random.choice(len(dataset_x_test_raw))\n","  dataset_x_test.append([dataset_x_test_raw[i1].reshape(28*28), dataset_x_test_raw[i2].reshape(28*28)])\n","  dataset_y_test.append(dataset_y_test_raw[i1] == dataset_y_test_raw[i2])\n","\n","dataset_x_test = np.array(dataset_x_test)\n","dataset_y_test = np.array(dataset_y_test)"],"metadata":{"id":"JCeVYm-sqA9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_x_train.shape, dataset_y_train.shape"],"metadata":{"id":"ebGVw5KrqyaH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El diseño de la red deberá, por tanto, procesar dos imágenes diferentes **con la misma capa** y luego transformar o combinar de alguna manera inteligente la salida de la capa para las dos salidas procesadas.\n","\n","Para ello, vamos a construir una capa que calcule la **Distancia Euclidea** entre dos vectores con Tensorflow."],"metadata":{"id":"Rx83tYVK1XOv"}},{"cell_type":"code","source":["class EuclDist(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super(EuclDist, self).__init__(**kwargs)\n","\n","    # La función build es la encargada de definir los pesos que deben ser entrenados\n","    # Si no hay pesos, no es necesaria\n","    def build(self, input_shape):\n","        pass\n","\n","    # La función call define la fase forward de la capa. Calcula la distancia euclidea\n","    # Hay un 1e-8 para evitar overflows con la raiz.\n","    def call(self, inputs):\n","        v1 = inputs[0]\n","        v2 = inputs[1]\n","        dist = tf.math.sqrt(tf.reduce_sum(tf.square(v1 - v2), axis=-1) + 1e-8)\n","        return tf.expand_dims(dist, axis=-1)"],"metadata":{"id":"kheNbTTamO0a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una vez hemos construido la nueva capa, vamos a programar la función de construcción de nuestro modelo. Aquí, un detalle importante es la inicialización de los pesos, que debe realizarse de manera explícita con una desviación más pequeña de lo normal."],"metadata":{"id":"YKu1qHuc1-Ik"}},{"cell_type":"code","source":["def build_model(input_dim, units, optimizer=\"Adam\"):\n","    ###########################################\n","    # TO-DO\n","    # Construye el modelo que reciba 2 entradas con dimension input_dim\n","    # y utilice la capa EuclDist que hemos diseñado previamente\n","    # Recuerda inicializar los pesos con stddev = 1e-4\n","    #\n","\n","\n","    #\n","    ###########################################\n","    return model"],"metadata":{"id":"M4K5u2KSmndq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A partir de aquí, el entrenamiento es normal teniendo en cuenta que hay dos vectores de entrada. Vamos a ver cómo se hace."],"metadata":{"id":"2f0sdYLB2uPX"}},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n","model = build_model(28*28, 50, optimizer=optimizer)\n","model.summary()"],"metadata":{"id":"tbkpqDvkn_nU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(model)"],"metadata":{"id":"03oYUZF5uuCr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hay que definir los vectores de entrada de forma independiente\n","x_train_v1 = dataset_x_train[:, 0, :]\n","x_train_v2 = dataset_x_train[:, 1, :]\n","x_test_v1 = dataset_x_test[:, 0, :]\n","x_test_v2 = dataset_x_test[:, 1, :]\n","\n","num_epochs = 10\n","loss_train = []\n","loss_test = []\n","acc_train = []\n","acc_test = []\n","for epoch in range(num_epochs):\n","  # En el fit, predict y evaluate debemos pasar como input una lista con todos\n","  # los vectores que el modelo necesita\n","  hist = model.fit([x_train_v1, x_train_v2], dataset_y_train,\n","                   validation_data = ([x_test_v1, x_test_v2], dataset_y_test))\n","  loss_train.append(hist.history[\"loss\"])\n","  loss_test.append(hist.history[\"val_loss\"])\n","  acc_train.append(hist.history[\"acc\"])\n","  acc_test.append(hist.history[\"val_acc\"])\n"],"metadata":{"id":"I0EcV4Bwoboy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10, 3))\n","plt.subplot(1,2,1)\n","plt.title(\"Loss\")\n","plt.plot(loss_train, label=\"Train\")\n","plt.plot(loss_test, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.subplot(1,2,2)\n","plt.title(\"Accuracy\")\n","plt.plot(acc_train, label=\"Train\")\n","plt.plot(acc_test, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"VFPzJcU2rxZx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio 2"],"metadata":{"id":"ixvATyV63Cy7"}},{"cell_type":"markdown","source":["El objetivo de este ejercicio consiste en diseñar una red que reciba dos imágenes y te diga si su diferencia (A - B) es mayor que 1, mayor que 2 o mayor que 3. Lo interesante aquí es que aquellas que sean mayor que 3, también serán mayor que 2 y mayor que 1, por tanto estamos en un **problema multietiqueta**.\n","\n","El modelo debe no solo aprender a identificar los números, sino también debe ser capaz de restar los dígitos que representa. Veamos si somos capaces de hacerlo.\n","\n","En primer lugar, construimos el dataset."],"metadata":{"id":"yMZySCG83K_S"}},{"cell_type":"code","source":["# En esta celda estamos construyendo el dataset nuevo. El procedimiento es el siguiente:\n","#  1. Selección aleatoria de dos imágenes de entrenamiento: imagen1 y imagen2\n","#  2. Guardamos:\n","#     |imagen1 - imagen2| > 1 como etiqueta1\n","#     |imagen1 - imagen2| > 2 como etiqueta2\n","#     |imagen1 - imagen2| > 3 como etiqueta3\n","\n","dataset_x_train = []\n","dataset_y_train = []\n","train_size = 50000\n","for i in range(train_size):\n","  i1 = np.random.choice(len(train_images))\n","  i2 = np.random.choice(len(train_images))\n","  dataset_x_train.append([train_images[i1].reshape(28*28), train_images[i2].reshape(28*28)])\n","  et1 = train_labels[i1].astype(int)\n","  et2 = train_labels[i2].astype(int)\n","  dataset_y_train.append([np.abs(et1 - et2) > 1, np.abs(et1 - et2) > 2, np.abs(et1 - et2) > 3])\n","\n","dataset_x_train = np.array(dataset_x_train)\n","dataset_y_train = np.array(dataset_y_train)\n","\n","# Mismo procedimiento para el test\n","\n","dataset_x_test = []\n","dataset_y_test = []\n","test_size = 20000\n","for i in range(test_size):\n","  i1 = np.random.choice(len(dataset_x_test_raw))\n","  i2 = np.random.choice(len(dataset_x_test_raw))\n","  dataset_x_test.append([test_images[i1].reshape(28*28), test_images[i2].reshape(28*28)])\n","  et1 = test_labels[i1].astype(int)\n","  et2 = test_labels[i2].astype(int)\n","  dataset_y_test.append([np.abs(et1 - et2) > 1, np.abs(et1 - et2) > 2, np.abs(et1 - et2) > 3])\n","\n","dataset_x_test = np.array(dataset_x_test)\n","dataset_y_test = np.array(dataset_y_test)"],"metadata":{"id":"QXv4kXch4yOy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_x_train.shape, dataset_y_train.shape"],"metadata":{"id":"-39GGYrB6GHI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a programar la función de construcción de nuestro modelo."],"metadata":{"id":"knjHotZLAhn4"}},{"cell_type":"code","source":["def build_model(input_dim, units1, units2, optimizer=\"Adam\"):\n","    ###########################################\n","    # TO-DO\n","    # Construye el modelo que reciba 2 entradas con dimension input_dim\n","    # Luego, utiliza una capa densa con units1 neuronas que procese las dos entradas\n","    # Luego, concatena las dos entradas usando tf.keras.layers.Concatenate()\n","    # Luego, utiliza una capa densa con units2 neuronas que procese el vector concatenado\n","    # Luego, construye 3 capas (para tomar las 3 decisiones) con activación sigmoid\n","    #\n","\n","\n","    #\n","    ###########################################\n","    return model"],"metadata":{"id":"s5dQWvSMyE8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n","model = build_model(28*28, 50, 50, optimizer=optimizer)\n","model.summary()"],"metadata":{"id":"DrFVI4rY4jfl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.plot_model(model)"],"metadata":{"id":"FZnEw7WX4mDw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hay que definir los vectores de entrada de forma independiente\n","x_train_v1 = dataset_x_train[:, 0, :]\n","x_train_v2 = dataset_x_train[:, 1, :]\n","x_test_v1 = dataset_x_test[:, 0, :]\n","x_test_v2 = dataset_x_test[:, 1, :]\n","\n","y_train_v1 = dataset_y_train[:, 0]\n","y_train_v2 = dataset_y_train[:, 1]\n","y_train_v3 = dataset_y_train[:, 2]\n","y_test_v1 = dataset_y_test[:, 0]\n","y_test_v2 = dataset_y_test[:, 1]\n","y_test_v3 = dataset_y_test[:, 2]\n","\n","\n","num_epochs = 20\n","loss_train = []\n","loss_test = []\n","acc_train_m1 = []\n","acc_train_m2 = []\n","acc_train_m3 = []\n","acc_test_m1 = []\n","acc_test_m2 = []\n","acc_test_m3 = []\n","for epoch in range(num_epochs):\n","  # En el fit, predict y evaluate debemos pasar como input una lista con todos\n","  # los vectores que el modelo necesita\n","  # Además, debemos pasar como output una lista con todos los vectores que el\n","  # modelo necesita\n","  hist = model.fit([x_train_v1, x_train_v2], [y_train_v1, y_train_v2, y_train_v3],\n","                   validation_data = ([x_test_v1, x_test_v2], [y_test_v1, y_test_v2, y_test_v3]))\n","  loss_train.append(hist.history[\"loss\"])\n","  loss_test.append(hist.history[\"val_loss\"])\n","  acc_train_m1.append(hist.history[\"m1_acc\"])\n","  acc_train_m2.append(hist.history[\"m2_acc\"])\n","  acc_train_m3.append(hist.history[\"m3_acc\"])\n","  acc_test_m1.append(hist.history[\"val_m1_acc\"])\n","  acc_test_m2.append(hist.history[\"val_m2_acc\"])\n","  acc_test_m3.append(hist.history[\"val_m3_acc\"])"],"metadata":{"id":"HuzlM_Jq4niu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15, 3))\n","plt.subplot(1,4,1)\n","plt.title(\"Loss\")\n","plt.plot(loss_train, label=\"Train\")\n","plt.plot(loss_test, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.subplot(1,4,2)\n","plt.title(\"Accuracy Mayor que 1\")\n","plt.plot(acc_train_m1, label=\"Train\")\n","plt.plot(acc_test_m1, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.subplot(1,4,3)\n","plt.title(\"Accuracy Mayor que 2\")\n","plt.plot(acc_train_m2, label=\"Train\")\n","plt.plot(acc_test_m2, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.subplot(1,4,4)\n","plt.title(\"Accuracy Mayor que 3\")\n","plt.plot(acc_train_m3, label=\"Train\")\n","plt.plot(acc_test_m3, label=\"Test\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"misEP6LX6Ycz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wALIl7HGErdB"},"execution_count":null,"outputs":[]}]}